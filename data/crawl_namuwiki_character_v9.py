#!/usr/bin/env python3
"""
ë‚˜ë¬´ìœ„í‚¤ ìºë¦­í„° í•œêµ­ì–´ ì´ë¦„ í¬ë¡¤ëŸ¬ v9
ì´ˆê³ ì† ë²„ì „ - httpx ì‚¬ìš© (Playwright ëŒ€ë¹„ 10ë°° ì´ìƒ ë¹ ë¦„)

- ë¸Œë¼ìš°ì € ì—†ì´ HTTP ìš”ì²­ë§Œ ì‚¬ìš©
- 20ê°œ ë™ì‹œ ìš”ì²­
- ì§„í–‰ ìƒí™© ì €ì¥ (ì¤‘ë‹¨ í›„ ì¬ê°œ ê°€ëŠ¥)
"""
import sys
import os
import re
import json
import asyncio
import traceback
from urllib.parse import quote, unquote
from datetime import datetime
from pathlib import Path

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'backend'))

from database import db

try:
    import httpx
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: httpxì™€ beautifulsoup4ê°€ í•„ìš”í•©ë‹ˆë‹¤")
    print("pip install httpx beautifulsoup4")
    sys.exit(1)


# Configuration
MAX_CONCURRENT = 15  # ë™ì‹œ ìš”ì²­ ìˆ˜
MAX_CHARACTERS = None  # None = ì „ì²´
REQUEST_TIMEOUT = 10  # ì´ˆ
MAX_RETRIES = 2

# ì§„í–‰ ìƒí™© íŒŒì¼
PROGRESS_FILE = Path(__file__).parent / "crawl_progress_v9.json"
ERROR_LOG_FILE = Path(__file__).parent / "crawl_errors_v9.log"

# Global counters
processed_count = 0
success_count = 0
error_count = 0
lock = asyncio.Lock()

# Rate limiting
semaphore = None


def log(msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}", flush=True)


def log_error(msg, error=None):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(ERROR_LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] {msg}\n")
        if error:
            f.write(f"    {error}\n")


def load_progress():
    if PROGRESS_FILE.exists():
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return {"processed_ids": [], "success": {}, "failed": []}


def save_progress(progress):
    try:
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log_error(f"Failed to save progress: {e}")


def get_characters_to_crawl(limit=None, exclude_ids=None):
    exclude_ids = exclude_ids or []

    query = """
        SELECT DISTINCT
            c.id,
            c.name_full,
            c.name_native,
            c.favourites
        FROM character c
        WHERE c.name_korean IS NULL
          AND c.name_native IS NOT NULL
          AND c.name_full NOT IN ('Narrator', 'Unknown', 'Extra')
          AND c.name_native != ''
          AND LENGTH(c.name_native) >= 2
    """

    if exclude_ids:
        placeholders = ",".join("?" * len(exclude_ids))
        query += f" AND c.id NOT IN ({placeholders})"

    query += " ORDER BY c.favourites DESC"

    if limit:
        query += f" LIMIT {limit}"

    if exclude_ids:
        return db.execute_query(query, tuple(exclude_ids))
    return db.execute_query(query)


def extract_kanji(text):
    return re.sub(r'[^\u4e00-\u9fff]', '', text)


def is_valid_korean_name(text):
    if not text:
        return False
    text = text.strip()
    if not re.match(r'^[ê°€-í£]+(\s[ê°€-í£]+)?$', text):
        return False
    clean = text.replace(' ', '')
    if len(clean) < 2 or len(clean) > 10:
        return False
    blacklist = ['ëª©ì°¨', 'ê°œìš”', 'ë“±ì¥ì¸ë¬¼', 'ì£¼ì¸ê³µ', 'ì„¤ëª…', 'íŠ¹ì§•', 'ë¶„ë¥˜',
                 'í¸ì§‘', 'í† ë¡ ', 'ì—­ì‚¬', 'ìµœê·¼', 'ìˆ˜ì •', 'ì‹œê°', 'ê¸°ëŠ¥',
                 'ì‘í’ˆ', 'ì‹œë¦¬ì¦ˆ', 'ì• ë‹ˆ', 'ë§Œí™”', 'ê²Œì„', 'ì†Œì„¤', 'ë”ë³´ê¸°',
                 'ì„±ìš°', 'ë°°ìš°', 'ì¶œìƒ', 'ì¶œì‹ ', 'ê¸°íƒ€', 'ê´€ê³„', 'ê°ì£¼', 'ëª©ë¡']
    if text in blacklist or any(b in text for b in blacklist):
        return False
    return True


def is_character_document(text):
    header = text[:4000]

    category_line = ""
    for line in header.split('\n')[:15]:
        if 'ë¶„ë¥˜' in line:
            category_line = line
            break

    real_person_cats = ['ë‚¨ë°°ìš°', 'ì—¬ë°°ìš°', 'ê°€ìˆ˜', 'ì•„ì´ëŒ',
                        'ì¶œìƒ', 'ë°ë·”', 'ì¶œì‹  ì¸ë¬¼', 'ì†Œì† ì—°ì˜ˆì¸']
    for cat in real_person_cats:
        if cat in category_line:
            return False

    lines = header.split('\n')
    for i, line in enumerate(lines):
        if line.strip() == 'ì§ì—…':
            if i + 1 < len(lines):
                next_line = lines[i + 1]
                if re.search(r'ë°°ìš°\s*\(\d{4}', next_line):
                    return False

    if 'ì„±ìš°' not in header:
        return False

    return True


def match_infobox_name(text, target_native, target_english):
    target_kanji = extract_kanji(target_native)

    if len(target_kanji) < 2:
        target_kanji = target_native

    english_full = target_english.lower().replace(' ', '')
    lines = text.split('\n')[:80]

    for line in lines:
        if ('/' in line or 'ï½œ' in line or '|' in line) and re.search(r'[A-Za-z]', line):
            line_kanji = extract_kanji(line)

            if not line_kanji or target_kanji != line_kanji:
                continue

            line_english = ''.join(re.findall(r'[A-Za-z]+', line)).lower()

            if english_full and line_english:
                common = sum(1 for c in english_full if c in line_english)
                if common >= len(english_full) * 0.7:
                    return True

            return True

    return False


def extract_korean_title(text):
    lines = text.split('\n')
    for line in lines[:20]:
        line = line.strip()
        if is_valid_korean_name(line):
            return line
    return None


async def fetch_page(client, url):
    """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ í¬í•¨)"""
    for attempt in range(MAX_RETRIES):
        try:
            async with semaphore:
                response = await client.get(url, timeout=REQUEST_TIMEOUT)
                if response.status_code == 200:
                    return response.text
                elif response.status_code == 429:
                    # Rate limited - ì ì‹œ ëŒ€ê¸°
                    await asyncio.sleep(2)
                    continue
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                await asyncio.sleep(1)
                continue
    return None


def extract_text_from_html(html):
    """HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, 'html.parser')

    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
    for tag in soup(['script', 'style', 'nav', 'footer']):
        tag.decompose()

    return soup.get_text(separator='\n', strip=True)


def extract_links_from_search(html):
    """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë§í¬ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, 'html.parser')
    links = []

    for a in soup.find_all('a', href=True):
        href = a['href']
        if href.startswith('/w/') and '#' not in href and ':' not in href:
            links.append(href)

    return list(dict.fromkeys(links))[:30]  # ì¤‘ë³µ ì œê±°, ìƒìœ„ 30ê°œ


async def search_and_find(client, search_term, name_native, name_full):
    """ê²€ìƒ‰ í›„ ìºë¦­í„° ì°¾ê¸°"""
    search_url = f"https://namu.wiki/Search?q={quote(search_term)}"

    html = await fetch_page(client, search_url)
    if not html:
        return None

    links = extract_links_from_search(html)

    # í•œê¸€ ì´ë¦„ URLë§Œ í•„í„°ë§
    korean_candidates = []
    for href in links:
        try:
            decoded = unquote(href.replace('/w/', ''))
            name_part = re.sub(r'\([^)]*\)$', '', decoded).strip()
            if is_valid_korean_name(name_part):
                korean_candidates.append({'name': name_part, 'href': href})
        except:
            pass

    # ì¤‘ë³µ ì œê±°
    seen = set()
    unique = []
    for c in korean_candidates:
        if c['name'] not in seen:
            seen.add(c['name'])
            unique.append(c)

    # ê° í›„ë³´ í™•ì¸ (ìƒìœ„ 5ê°œë§Œ)
    for candidate in unique[:5]:
        korean_name = candidate['name']
        href = candidate['href']

        doc_url = f"https://namu.wiki{href}"
        doc_html = await fetch_page(client, doc_url)

        if not doc_html:
            continue

        text = extract_text_from_html(doc_html)

        if "í•´ë‹¹ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in text:
            continue

        if not is_character_document(text):
            continue

        if match_infobox_name(text, name_native, name_full):
            title = extract_korean_title(text)
            if title:
                return title
            return korean_name

    return None


async def find_korean_name(client, name_native, name_full):
    """ë‚˜ë¬´ìœ„í‚¤ì—ì„œ í•œêµ­ì–´ ì´ë¦„ ì°¾ê¸°"""

    # 1. ì¼ë³¸ì–´ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
    result = await search_and_find(client, name_native, name_native, name_full)
    if result:
        return result

    # 2. ì˜ì–´ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
    result = await search_and_find(client, name_full, name_native, name_full)
    if result:
        return result

    # 3. ì˜ì–´ ì´ë¦„ ì¼ë¶€ë¡œ ê²€ìƒ‰ (ì„±ë§Œ)
    parts = name_full.split()
    if len(parts) >= 2:
        result = await search_and_find(client, parts[-1], name_native, name_full)
        if result:
            return result

    return None


async def process_character(client, character, total_count, progress):
    """ë‹¨ì¼ ìºë¦­í„° ì²˜ë¦¬"""
    global processed_count, success_count, error_count

    char_id = character['id']
    name_full = character['name_full']
    name_native = character['name_native']

    korean_name = None
    error_occurred = False

    try:
        korean_name = await find_korean_name(client, name_native, name_full)

        if korean_name:
            db.execute_update(
                "UPDATE character SET name_korean = ? WHERE id = ?",
                (korean_name, char_id)
            )

    except Exception as e:
        error_occurred = True
        log_error(f"Error on {name_full}: {e}")

    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
    async with lock:
        processed_count += 1
        current = processed_count

        if korean_name:
            success_count += 1
            progress["success"][str(char_id)] = korean_name
            log(f"âœ“ [{current}/{total_count}] {name_full} â†’ {korean_name}")
        elif error_occurred:
            error_count += 1
            progress["failed"].append(char_id)
            log(f"âš  [{current}/{total_count}] {name_full} (ì—ëŸ¬)")
        else:
            log(f"âœ— [{current}/{total_count}] {name_full}")

        progress["processed_ids"].append(char_id)

        # 50ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì €ì¥
        if current % 50 == 0:
            rate = success_count / current * 100 if current > 0 else 0
            log(f"\n{'='*60}")
            log(f"ğŸ“Š ì§„í–‰ìƒí™©: {current}/{total_count} ({current/total_count*100:.1f}%)")
            log(f"   ì„±ê³µ: {success_count}ê°œ ({rate:.1f}%)")
            log(f"   ì‹¤íŒ¨: {current - success_count - error_count}ê°œ")
            log(f"   ì—ëŸ¬: {error_count}ê°œ")
            log(f"{'='*60}\n")
            save_progress(progress)


async def main():
    global processed_count, success_count, error_count, semaphore

    log("=" * 60)
    log("ğŸš€ ë‚˜ë¬´ìœ„í‚¤ ìºë¦­í„° í•œêµ­ì–´ ì´ë¦„ í¬ë¡¤ëŸ¬ v9")
    log(f"   ì´ˆê³ ì† httpx ë²„ì „ - {MAX_CONCURRENT}ê°œ ë™ì‹œ ìš”ì²­")
    log("=" * 60)

    # Semaphore ì´ˆê¸°í™”
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)

    # ì´ì „ ì§„í–‰ ìƒí™© ë¡œë“œ
    progress = load_progress()
    processed_ids = progress.get("processed_ids", [])

    if processed_ids:
        log(f"\nğŸ“‚ ì´ì „ ì§„í–‰ ìƒí™© ë°œê²¬: {len(processed_ids)}ê°œ ì²˜ë¦¬ë¨")
        log(f"   ì„±ê³µ: {len(progress.get('success', {}))}ê°œ")
        log(f"   ì´ì–´ì„œ ì§„í–‰í•©ë‹ˆë‹¤...\n")

    log("\nğŸ“‹ ì²˜ë¦¬í•  ìºë¦­í„° ì¡°íšŒ ì¤‘...")
    characters = get_characters_to_crawl(limit=MAX_CHARACTERS, exclude_ids=processed_ids)
    total_count = len(characters)

    log(f"   ì´ {total_count}ê°œ ìºë¦­í„° ë°œê²¬")

    if total_count == 0:
        log("âœ… ì²˜ë¦¬í•  ìºë¦­í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    processed_count = 0
    success_count = 0
    error_count = 0

    log(f"\nğŸ”„ í¬ë¡¤ë§ ì‹œì‘...")
    start_time = datetime.now()

    # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    }

    async with httpx.AsyncClient(headers=headers, follow_redirects=True) as client:
        # ë°°ì¹˜ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        batch_size = 100
        for i in range(0, len(characters), batch_size):
            batch = characters[i:i + batch_size]

            tasks = [
                process_character(client, char, total_count, progress)
                for char in batch
            ]

            await asyncio.gather(*tasks)

            # ë°°ì¹˜ ì™„ë£Œ í›„ ì ì‹œ ëŒ€ê¸° (rate limiting)
            await asyncio.sleep(1)

    # ìµœì¢… ì§„í–‰ ìƒí™© ì €ì¥
    save_progress(progress)

    elapsed = (datetime.now() - start_time).total_seconds()

    log(f"\n\n{'='*60}")
    log("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    log(f"{'='*60}")
    log(f"  ì´ ì²˜ë¦¬: {processed_count}ê°œ")
    if processed_count > 0:
        log(f"  ì„±ê³µ: {success_count}ê°œ ({success_count/processed_count*100:.1f}%)")
        log(f"  ì‹¤íŒ¨: {processed_count - success_count - error_count}ê°œ")
        log(f"  ì—ëŸ¬: {error_count}ê°œ")
    log(f"  ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„")
    log(f"  ì†ë„: {processed_count / elapsed * 60:.1f}ê°œ/ë¶„")
    log(f"  ì§„í–‰ ìƒí™© ì €ì¥: {PROGRESS_FILE}")
    log(f"{'='*60}\n")


if __name__ == "__main__":
    asyncio.run(main())
