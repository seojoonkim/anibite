#!/usr/bin/env python3
"""
êµ¬ê¸€ ê²€ìƒ‰ ê¸°ë°˜ ìºë¦­í„° í•œêµ­ì–´ ì´ë¦„ í¬ë¡¤ëŸ¬
ê°€ì¥ ë¹ ë¥´ê³  ê°„ë‹¨í•œ ë°©ì‹

ê²€ìƒ‰ ì¿¼ë¦¬: "ìºë¦­í„° ì˜ì–´ì´ë¦„" + "ì´ë¦„" ë˜ëŠ” site:namu.wiki
ì²« ë²ˆì§¸ ê²°ê³¼ì—ì„œ í•œêµ­ì–´ ì´ë¦„ ì¶”ì¶œ
"""
import sys
import os
import re
import json
import asyncio
from datetime import datetime
from pathlib import Path
from urllib.parse import unquote, quote

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'backend'))

from database import db

try:
    import httpx
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: httpxì™€ beautifulsoup4ê°€ í•„ìš”í•©ë‹ˆë‹¤")
    print("pip install httpx beautifulsoup4")
    sys.exit(1)


# Configuration
MAX_CONCURRENT = 5  # êµ¬ê¸€ì€ rate limitì´ ë¹¡ì„¸ì„œ ì ê²Œ
MAX_CHARACTERS = None
REQUEST_TIMEOUT = 15

PROGRESS_FILE = Path(__file__).parent / "crawl_progress_google.json"
ERROR_LOG_FILE = Path(__file__).parent / "crawl_errors_google.log"

# Global
processed_count = 0
success_count = 0
error_count = 0
lock = asyncio.Lock()
semaphore = None


def log(msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}", flush=True)


def log_error(msg):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(ERROR_LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] {msg}\n")


def load_progress():
    if PROGRESS_FILE.exists():
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return {"processed_ids": [], "success": {}, "failed": []}


def save_progress(progress):
    try:
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    except:
        pass


def get_characters_to_crawl(limit=None, exclude_ids=None):
    exclude_ids = exclude_ids or []

    query = """
        SELECT DISTINCT
            c.id,
            c.name_full,
            c.name_native,
            c.favourites
        FROM character c
        WHERE c.name_korean IS NULL
          AND c.name_native IS NOT NULL
          AND c.name_full NOT IN ('Narrator', 'Unknown', 'Extra')
          AND c.name_native != ''
          AND LENGTH(c.name_native) >= 2
    """

    if exclude_ids:
        placeholders = ",".join("?" * len(exclude_ids))
        query += f" AND c.id NOT IN ({placeholders})"

    query += " ORDER BY c.favourites DESC"

    if limit:
        query += f" LIMIT {limit}"

    if exclude_ids:
        return db.execute_query(query, tuple(exclude_ids))
    return db.execute_query(query)


def is_valid_korean_name(text):
    if not text:
        return False
    text = text.strip()
    if not re.match(r'^[ê°€-í£]+(\s[ê°€-í£]+)?$', text):
        return False
    clean = text.replace(' ', '')
    if len(clean) < 2 or len(clean) > 10:
        return False
    blacklist = ['ëª©ì°¨', 'ê°œìš”', 'ë“±ì¥ì¸ë¬¼', 'ì£¼ì¸ê³µ', 'ì„¤ëª…', 'íŠ¹ì§•', 'ë¶„ë¥˜',
                 'í¸ì§‘', 'í† ë¡ ', 'ì—­ì‚¬', 'ìµœê·¼', 'ìˆ˜ì •', 'ì‹œê°', 'ê¸°ëŠ¥',
                 'ì‘í’ˆ', 'ì‹œë¦¬ì¦ˆ', 'ì• ë‹ˆ', 'ë§Œí™”', 'ê²Œì„', 'ì†Œì„¤', 'ë”ë³´ê¸°',
                 'ì„±ìš°', 'ë°°ìš°', 'ì¶œìƒ', 'ì¶œì‹ ', 'ê¸°íƒ€', 'ê´€ê³„', 'ê°ì£¼', 'ëª©ë¡',
                 'ë‚˜ë¬´ìœ„í‚¤', 'ê²€ìƒ‰', 'ê²°ê³¼']
    if text in blacklist or any(b in text for b in blacklist):
        return False
    return True


def extract_korean_from_url(url):
    """URLì—ì„œ í•œêµ­ì–´ ì´ë¦„ ì¶”ì¶œ (ë‚˜ë¬´ìœ„í‚¤ URL íŒ¨í„´)"""
    # namu.wiki/w/ì—˜ëŸ° ì˜ˆê±° í˜•íƒœ
    if 'namu.wiki/w/' in url:
        try:
            path = url.split('/w/')[-1]
            decoded = unquote(path)
            # ê´„í˜¸ ì œê±° (ë™ëª…ì´ì¸ êµ¬ë¶„ìš©)
            name = re.sub(r'\([^)]*\)$', '', decoded).strip()
            if is_valid_korean_name(name):
                return name
        except:
            pass
    return None


def extract_korean_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ í•œêµ­ì–´ ì´ë¦„ ì¶”ì¶œ"""
    # "XXX - ë‚˜ë¬´ìœ„í‚¤" íŒ¨í„´
    match = re.search(r'^([ê°€-í£]+(?:\s[ê°€-í£]+)?)\s*[-â€“â€”]\s*ë‚˜ë¬´ìœ„í‚¤', text)
    if match:
        name = match.group(1).strip()
        if is_valid_korean_name(name):
            return name

    # "XXX(í•œê¸€) - " íŒ¨í„´
    match = re.search(r'([ê°€-í£]{2,10})\s*\(', text)
    if match:
        name = match.group(1).strip()
        if is_valid_korean_name(name):
            return name

    return None


async def search_google(client, query):
    """êµ¬ê¸€ ê²€ìƒ‰ ìˆ˜í–‰"""
    url = f"https://www.google.com/search?q={quote(query)}&hl=ko&gl=kr"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    }

    try:
        async with semaphore:
            response = await client.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            if response.status_code == 200:
                return response.text
            elif response.status_code == 429:
                log("âš  êµ¬ê¸€ Rate limit ê°ì§€, 30ì´ˆ ëŒ€ê¸°...")
                await asyncio.sleep(30)
    except Exception as e:
        log_error(f"Search error: {e}")

    return None


def parse_google_results(html, name_native, name_full):
    """êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
    soup = BeautifulSoup(html, 'html.parser')

    # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‚˜ë¬´ìœ„í‚¤ ë§í¬ ì°¾ê¸°
    for a in soup.find_all('a', href=True):
        href = a.get('href', '')

        # ë‚˜ë¬´ìœ„í‚¤ URL ì°¾ê¸°
        if 'namu.wiki/w/' in href:
            korean_name = extract_korean_from_url(href)
            if korean_name:
                return korean_name

        # ì œëª©ì—ì„œ í•œêµ­ì–´ ì´ë¦„ ì¶”ì¶œ
        title = a.get_text()
        if 'ë‚˜ë¬´ìœ„í‚¤' in title:
            korean_name = extract_korean_from_text(title)
            if korean_name:
                return korean_name

    # ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí«ì—ì„œ ì°¾ê¸°
    for div in soup.find_all(['div', 'span']):
        text = div.get_text()

        # "XXX(í•œêµ­ì–´ë°œìŒ)" íŒ¨í„´
        # ì˜ˆ: "ã‚¨ãƒ¬ãƒ³ãƒ»ã‚¤ã‚§ãƒ¼ã‚¬ãƒ¼(ì—˜ëŸ° ì˜ˆê±°)"
        pattern = rf'{re.escape(name_native[:3])}.*?\(([ê°€-í£]{{2,10}})\)'
        match = re.search(pattern, text)
        if match:
            name = match.group(1)
            if is_valid_korean_name(name):
                return name

    return None


async def find_korean_name_google(client, name_native, name_full):
    """êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ í•œêµ­ì–´ ì´ë¦„ ì°¾ê¸°"""

    # ì „ëµ 1: ì¼ë³¸ì–´ ì´ë¦„ + site:namu.wiki
    query = f'"{name_native}" site:namu.wiki'
    html = await search_google(client, query)
    if html:
        result = parse_google_results(html, name_native, name_full)
        if result:
            return result

    await asyncio.sleep(1)  # êµ¬ê¸€ rate limit ëŒ€ì‘

    # ì „ëµ 2: ì˜ì–´ ì´ë¦„ + "ì´ë¦„" + ì• ë‹ˆë©”ì´ì…˜
    query = f'{name_full} ì´ë¦„ ìºë¦­í„°'
    html = await search_google(client, query)
    if html:
        result = parse_google_results(html, name_native, name_full)
        if result:
            return result

    await asyncio.sleep(1)

    # ì „ëµ 3: ì˜ì–´ ì´ë¦„ + site:namu.wiki
    query = f'"{name_full}" site:namu.wiki'
    html = await search_google(client, query)
    if html:
        result = parse_google_results(html, name_native, name_full)
        if result:
            return result

    return None


async def process_character(client, character, total_count, progress):
    """ë‹¨ì¼ ìºë¦­í„° ì²˜ë¦¬"""
    global processed_count, success_count, error_count

    char_id = character['id']
    name_full = character['name_full']
    name_native = character['name_native']

    korean_name = None
    error_occurred = False

    try:
        korean_name = await find_korean_name_google(client, name_native, name_full)

        if korean_name:
            db.execute_update(
                "UPDATE character SET name_korean = ? WHERE id = ?",
                (korean_name, char_id)
            )

    except Exception as e:
        error_occurred = True
        log_error(f"Error on {name_full}: {e}")

    async with lock:
        processed_count += 1
        current = processed_count

        if korean_name:
            success_count += 1
            progress["success"][str(char_id)] = korean_name
            log(f"âœ“ [{current}/{total_count}] {name_full} â†’ {korean_name}")
        elif error_occurred:
            error_count += 1
            progress["failed"].append(char_id)
            log(f"âš  [{current}/{total_count}] {name_full} (ì—ëŸ¬)")
        else:
            log(f"âœ— [{current}/{total_count}] {name_full}")

        progress["processed_ids"].append(char_id)

        if current % 20 == 0:
            rate = success_count / current * 100 if current > 0 else 0
            log(f"\n{'='*60}")
            log(f"ğŸ“Š ì§„í–‰ìƒí™©: {current}/{total_count} ({current/total_count*100:.1f}%)")
            log(f"   ì„±ê³µ: {success_count}ê°œ ({rate:.1f}%)")
            log(f"   ì—ëŸ¬: {error_count}ê°œ")
            log(f"{'='*60}\n")
            save_progress(progress)


async def main():
    global processed_count, success_count, error_count, semaphore

    log("=" * 60)
    log("ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ê¸°ë°˜ ìºë¦­í„° í•œêµ­ì–´ ì´ë¦„ í¬ë¡¤ëŸ¬")
    log(f"   ë™ì‹œ ìš”ì²­: {MAX_CONCURRENT}ê°œ")
    log("=" * 60)

    semaphore = asyncio.Semaphore(MAX_CONCURRENT)

    progress = load_progress()
    processed_ids = progress.get("processed_ids", [])

    if processed_ids:
        log(f"\nğŸ“‚ ì´ì „ ì§„í–‰ ìƒí™©: {len(processed_ids)}ê°œ ì²˜ë¦¬ë¨")
        log(f"   ì„±ê³µ: {len(progress.get('success', {}))}ê°œ")

    log("\nğŸ“‹ ì²˜ë¦¬í•  ìºë¦­í„° ì¡°íšŒ ì¤‘...")
    characters = get_characters_to_crawl(limit=MAX_CHARACTERS, exclude_ids=processed_ids)
    total_count = len(characters)

    log(f"   ì´ {total_count}ê°œ ìºë¦­í„°")

    if total_count == 0:
        log("âœ… ì²˜ë¦¬í•  ìºë¦­í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    processed_count = 0
    success_count = 0
    error_count = 0

    log(f"\nğŸ”„ í¬ë¡¤ë§ ì‹œì‘...")
    start_time = datetime.now()

    async with httpx.AsyncClient(follow_redirects=True) as client:
        # ìˆœì°¨ ì²˜ë¦¬ (êµ¬ê¸€ rate limit ë•Œë¬¸ì—)
        for char in characters:
            await process_character(client, char, total_count, progress)
            await asyncio.sleep(2)  # êµ¬ê¸€ rate limit ëŒ€ì‘

    save_progress(progress)

    elapsed = (datetime.now() - start_time).total_seconds()

    log(f"\n\n{'='*60}")
    log("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    log(f"{'='*60}")
    log(f"  ì´ ì²˜ë¦¬: {processed_count}ê°œ")
    if processed_count > 0:
        log(f"  ì„±ê³µ: {success_count}ê°œ ({success_count/processed_count*100:.1f}%)")
        log(f"  ì—ëŸ¬: {error_count}ê°œ")
    log(f"  ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„")
    log(f"  ì†ë„: {processed_count / elapsed * 60:.1f}ê°œ/ë¶„")
    log(f"{'='*60}\n")


if __name__ == "__main__":
    asyncio.run(main())
