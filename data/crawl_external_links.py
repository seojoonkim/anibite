"""
ì™¸ë¶€ ë§í¬ ë°ì´í„° í¬ë¡¤ë§
ì• ë‹ˆë©”ì´ì…˜ì˜ ê³µì‹ ì‚¬ì´íŠ¸, íŠ¸ìœ„í„°, ìŠ¤íŠ¸ë¦¬ë° ë§í¬ ë“± ìˆ˜ì§‘
"""
import os
import sys

from crawler import AnimeCrawler

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ğŸ”— ì™¸ë¶€ ë§í¬ ë°ì´í„° í¬ë¡¤ë§                               â•‘
â•‘   ëŒ€ìƒ: ìƒìœ„ 3,000ê°œ ì• ë‹ˆë©”ì´ì…˜                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    crawler = AnimeCrawler()
    crawler.connect()

    try:
        # ì´ë¯¸ ì• ë‹ˆë©”ì´ì…˜ ë°ì´í„°ê°€ ìˆëŠ” ê²ƒë“¤ì— ëŒ€í•´ ì™¸ë¶€ ë§í¬ ì¶”ê°€
        cursor = crawler.conn.cursor()
        cursor.execute("""
            SELECT id, title_romaji
            FROM anime
            ORDER BY popularity DESC
            LIMIT 3000
        """)

        anime_list = cursor.fetchall()
        total = len(anime_list)

        print(f"ğŸ“Š í¬ë¡¤ë§ ëŒ€ìƒ: {total}ê°œ ì• ë‹ˆë©”ì´ì…˜\n")

        success_count = 0
        error_count = 0
        total_links = 0

        for i, (anime_id, title) in enumerate(anime_list, 1):
            print(f"\n[{i}/{total}] {title} (ID: {anime_id})")

            try:
                # ì™¸ë¶€ ë§í¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                query = """
                query ($id: Int) {
                    Media(id: $id) {
                        externalLinks {
                            url
                            site
                            type
                            language
                        }
                    }
                }
                """

                variables = {'id': anime_id}
                response = crawler.client._make_request(query, variables)

                if response and 'Media' in response:
                    external_links = response['Media'].get('externalLinks', [])
                    link_count = 0

                    for link in external_links:
                        url = link.get('url')
                        site = link.get('site')
                        link_type = link.get('type')
                        language = link.get('language')

                        if url:
                            # ì™¸ë¶€ ë§í¬ ì €ì¥
                            crawler.conn.execute("""
                                INSERT OR IGNORE INTO anime_external_link (
                                    anime_id, url, site, type, language
                                ) VALUES (?, ?, ?, ?, ?)
                            """, (
                                anime_id, url, site, link_type, language
                            ))

                            link_count += 1

                    crawler.conn.commit()
                    success_count += 1
                    total_links += link_count
                    print(f"  âœ… ë§í¬ {link_count}ê°œ ì €ì¥")

                time.sleep(1)  # Rate limiting

            except Exception as e:
                error_count += 1
                print(f"  âŒ ì—ëŸ¬: {e}")
                continue

        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ì„±ê³µ: {success_count}ê°œ")
        print(f"  ì‹¤íŒ¨: {error_count}ê°œ")
        print(f"  ì´ ë§í¬: {total_links}ê°œ")
        print(f"{'='*60}\n")

    finally:
        crawler.close()

if __name__ == '__main__':
    import time
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
