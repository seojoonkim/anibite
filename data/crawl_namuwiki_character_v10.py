#!/usr/bin/env python3
"""
ë‚˜ë¬´ìœ„í‚¤ ìºë¦­í„° í•œêµ­ì–´ ì´ë¦„ í¬ë¡¤ëŸ¬ v10
ì§§ì€ ì´ë¦„ ìºë¦­í„° ì „ìš© - ì• ë‹ˆë©”ì´ì…˜ ì œëª©ê³¼ í•¨ê»˜ ê²€ìƒ‰

ì˜ˆ: "Jin ì‚¬ë¬´ë¼ì´ ì°¸í”„ë£¨" â†’ ì§„
"""
import sys
import os
import re
import json
import asyncio
from urllib.parse import quote, unquote
from datetime import datetime
from pathlib import Path

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'backend'))

from database import db

try:
    import httpx
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: pip install httpx beautifulsoup4")
    sys.exit(1)


# Configuration
MAX_CONCURRENT = 10
MAX_CHARACTERS = None
REQUEST_TIMEOUT = 10

PROGRESS_FILE = Path(__file__).parent / "crawl_progress_v10.json"
ERROR_LOG_FILE = Path(__file__).parent / "crawl_errors_v10.log"

# Global
processed_count = 0
success_count = 0
lock = asyncio.Lock()
semaphore = None


def log(msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}", flush=True)


def load_progress():
    if PROGRESS_FILE.exists():
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return {"processed_ids": [], "success": {}, "failed": []}


def save_progress(progress):
    try:
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    except:
        pass


def get_characters_with_anime(limit=None, exclude_ids=None):
    """ì• ë‹ˆë©”ì´ì…˜ ì •ë³´ì™€ í•¨ê»˜ ìºë¦­í„° ì¡°íšŒ"""
    exclude_ids = exclude_ids or []

    query = """
        SELECT DISTINCT
            c.id,
            c.name_full,
            c.name_native,
            c.favourites,
            (
                SELECT a.title_romaji
                FROM anime a
                JOIN anime_character ac ON a.id = ac.anime_id
                WHERE ac.character_id = c.id
                LIMIT 1
            ) as anime_title,
            (
                SELECT a.title_korean
                FROM anime a
                JOIN anime_character ac ON a.id = ac.anime_id
                WHERE ac.character_id = c.id
                AND a.title_korean IS NOT NULL
                LIMIT 1
            ) as anime_title_korean
        FROM character c
        WHERE c.name_korean IS NULL
          AND c.name_native IS NOT NULL
          AND c.name_full NOT IN ('Narrator', 'Unknown', 'Extra')
          AND c.name_native != ''
    """

    if exclude_ids:
        placeholders = ",".join("?" * len(exclude_ids))
        query += f" AND c.id NOT IN ({placeholders})"

    query += " ORDER BY c.favourites DESC"

    if limit:
        query += f" LIMIT {limit}"

    if exclude_ids:
        return db.execute_query(query, tuple(exclude_ids))
    return db.execute_query(query)


def is_valid_korean_name(text):
    if not text:
        return False
    text = text.strip()
    if not re.match(r'^[ê°€-í£]+(\s[ê°€-í£]+)?$', text):
        return False
    clean = text.replace(' ', '')
    if len(clean) < 1 or len(clean) > 10:  # 1ê¸€ìë„ í—ˆìš© (ì§§ì€ ì´ë¦„ìš©)
        return False
    blacklist = ['ëª©ì°¨', 'ê°œìš”', 'ë“±ì¥ì¸ë¬¼', 'ì£¼ì¸ê³µ', 'ì„¤ëª…', 'íŠ¹ì§•', 'ë¶„ë¥˜',
                 'í¸ì§‘', 'í† ë¡ ', 'ì—­ì‚¬', 'ìµœê·¼', 'ìˆ˜ì •', 'ì‹œê°', 'ê¸°ëŠ¥',
                 'ì‘í’ˆ', 'ì‹œë¦¬ì¦ˆ', 'ì• ë‹ˆ', 'ë§Œí™”', 'ê²Œì„', 'ì†Œì„¤', 'ë”ë³´ê¸°',
                 'ì„±ìš°', 'ë°°ìš°', 'ì¶œìƒ', 'ì¶œì‹ ', 'ê¸°íƒ€', 'ê´€ê³„', 'ê°ì£¼', 'ëª©ë¡']
    if text in blacklist or any(b in text for b in blacklist):
        return False
    return True


def extract_kanji(text):
    return re.sub(r'[^\u4e00-\u9fff]', '', text)


async def fetch_page(client, url):
    """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
    try:
        async with semaphore:
            response = await client.get(url, timeout=REQUEST_TIMEOUT)
            if response.status_code == 200:
                return response.text
    except:
        pass
    return None


def extract_links_from_search(html):
    """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë§í¬ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, 'html.parser')
    links = []

    for a in soup.find_all('a', href=True):
        href = a['href']
        if href.startswith('/w/') and '#' not in href and ':' not in href:
            links.append(href)

    return list(dict.fromkeys(links))[:30]


def is_character_document(text, name_native):
    """ìºë¦­í„° ë¬¸ì„œì¸ì§€ í™•ì¸ (ì§§ì€ ì´ë¦„ìš© ì™„í™”ëœ ë²„ì „)"""
    header = text[:3000]

    # ì‹¤ì¡´ ì¸ë¬¼ ì œì™¸
    real_person_cats = ['ë‚¨ë°°ìš°', 'ì—¬ë°°ìš°', 'ê°€ìˆ˜', 'ì•„ì´ëŒ', 'ì¶œìƒ', 'ë°ë·”']
    for cat in real_person_cats:
        if cat in header[:1000]:
            return False

    # ì¼ë³¸ì–´ ì´ë¦„ì´ ìˆìœ¼ë©´ ìºë¦­í„°ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
    if name_native in text[:2000]:
        return True

    # ì„±ìš° ì •ë³´ê°€ ìˆìœ¼ë©´ ìºë¦­í„°
    if 'ì„±ìš°' in header or 'CV' in header or 'C.V' in header:
        return True

    return False


def match_character_name(text, name_native, name_full):
    """ìºë¦­í„° ì´ë¦„ ë§¤ì¹­ í™•ì¸"""
    header = text[:2000]

    # ì¼ë³¸ì–´ ì´ë¦„ ì§ì ‘ ë§¤ì¹­
    if name_native in header:
        return True

    # í•œì ì¶”ì¶œí•´ì„œ ë§¤ì¹­
    target_kanji = extract_kanji(name_native)
    if target_kanji and target_kanji in header:
        return True

    # ì˜ì–´ ì´ë¦„ ë§¤ì¹­
    if name_full.lower() in header.lower():
        return True

    return False


def extract_korean_title(text):
    """í˜ì´ì§€ì—ì„œ í•œê¸€ ì œëª© ì¶”ì¶œ"""
    lines = text.split('\n')
    for line in lines[:15]:
        line = line.strip()
        # í•œê¸€ë§Œ ìˆëŠ” ë¼ì¸
        if re.match(r'^[ê°€-í£]+$', line) and 1 <= len(line) <= 10:
            if is_valid_korean_name(line):
                return line
    return None


async def search_with_anime(client, name_native, name_full, anime_title, anime_title_korean):
    """ì• ë‹ˆë©”ì´ì…˜ ì œëª©ê³¼ í•¨ê»˜ ê²€ìƒ‰"""

    search_terms = []

    # 1. ì¼ë³¸ì–´ ì´ë¦„ + í•œêµ­ì–´ ì• ë‹ˆ ì œëª©
    if anime_title_korean:
        search_terms.append(f"{name_native} {anime_title_korean}")

    # 2. ì¼ë³¸ì–´ ì´ë¦„ë§Œ
    search_terms.append(name_native)

    # 3. ì˜ì–´ ì´ë¦„ + ì˜ì–´ ì• ë‹ˆ ì œëª©
    if anime_title:
        search_terms.append(f"{name_full} {anime_title}")

    for search_term in search_terms:
        search_url = f"https://namu.wiki/Search?q={quote(search_term)}"

        html = await fetch_page(client, search_url)
        if not html:
            continue

        links = extract_links_from_search(html)

        # í•œê¸€ ì´ë¦„ URL í•„í„°ë§
        korean_candidates = []
        for href in links:
            try:
                decoded = unquote(href.replace('/w/', ''))
                name_part = re.sub(r'\([^)]*\)$', '', decoded).strip()
                if is_valid_korean_name(name_part):
                    korean_candidates.append({'name': name_part, 'href': href})
            except:
                pass

        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique = []
        for c in korean_candidates:
            if c['name'] not in seen:
                seen.add(c['name'])
                unique.append(c)

        # ê° í›„ë³´ í™•ì¸
        for candidate in unique[:5]:
            korean_name = candidate['name']
            href = candidate['href']

            doc_url = f"https://namu.wiki{href}"
            doc_html = await fetch_page(client, doc_url)

            if not doc_html:
                continue

            soup = BeautifulSoup(doc_html, 'html.parser')
            text = soup.get_text(separator='\n', strip=True)

            if "í•´ë‹¹ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in text:
                continue

            if not is_character_document(text, name_native):
                continue

            if match_character_name(text, name_native, name_full):
                title = extract_korean_title(text)
                if title:
                    return title
                return korean_name

        await asyncio.sleep(0.3)

    return None


async def process_character(client, character, total_count, progress):
    """ë‹¨ì¼ ìºë¦­í„° ì²˜ë¦¬"""
    global processed_count, success_count

    char_id = character['id']
    name_full = character['name_full']
    name_native = character['name_native']
    anime_title = character['anime_title'] or ''
    anime_title_korean = character['anime_title_korean'] or ''

    korean_name = None

    try:
        korean_name = await search_with_anime(
            client, name_native, name_full, anime_title, anime_title_korean
        )

        if korean_name:
            db.execute_update(
                "UPDATE character SET name_korean = ? WHERE id = ?",
                (korean_name, char_id)
            )

    except Exception as e:
        pass

    async with lock:
        processed_count += 1
        current = processed_count

        if korean_name:
            success_count += 1
            progress["success"][str(char_id)] = korean_name
            anime_info = f" [{anime_title_korean or anime_title}]" if anime_title else ""
            log(f"âœ“ [{current}/{total_count}] {name_full}{anime_info} â†’ {korean_name}")
        else:
            log(f"âœ— [{current}/{total_count}] {name_full} ({name_native})")

        progress["processed_ids"].append(char_id)

        if current % 20 == 0:
            rate = success_count / current * 100 if current > 0 else 0
            log(f"\nğŸ“Š ì§„í–‰: {current}/{total_count} | ì„±ê³µ: {success_count}ê°œ ({rate:.1f}%)\n")
            save_progress(progress)


async def main():
    global processed_count, success_count, semaphore

    log("=" * 60)
    log("ğŸš€ ë‚˜ë¬´ìœ„í‚¤ ìºë¦­í„° í¬ë¡¤ëŸ¬ v10")
    log("   ì§§ì€ ì´ë¦„ ìºë¦­í„° ì „ìš© - ì• ë‹ˆë©”ì´ì…˜ ì œëª©ê³¼ í•¨ê»˜ ê²€ìƒ‰")
    log("=" * 60)

    semaphore = asyncio.Semaphore(MAX_CONCURRENT)

    progress = load_progress()
    processed_ids = progress.get("processed_ids", [])

    if processed_ids:
        log(f"\nğŸ“‚ ì´ì „ ì§„í–‰: {len(processed_ids)}ê°œ ì²˜ë¦¬ë¨, ì„±ê³µ: {len(progress.get('success', {}))}ê°œ")

    log("\nğŸ“‹ ìºë¦­í„° ì¡°íšŒ ì¤‘...")
    characters = get_characters_with_anime(limit=MAX_CHARACTERS, exclude_ids=processed_ids)
    total_count = len(characters)

    log(f"   ì´ {total_count}ê°œ ìºë¦­í„°")

    if total_count == 0:
        log("âœ… ì²˜ë¦¬í•  ìºë¦­í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    processed_count = 0
    success_count = 0

    log(f"\nğŸ”„ í¬ë¡¤ë§ ì‹œì‘...")
    start_time = datetime.now()

    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Accept-Language': 'ko-KR,ko;q=0.9',
    }

    async with httpx.AsyncClient(headers=headers, follow_redirects=True) as client:
        batch_size = 50
        for i in range(0, len(characters), batch_size):
            batch = characters[i:i + batch_size]

            tasks = [
                process_character(client, char, total_count, progress)
                for char in batch
            ]

            await asyncio.gather(*tasks)
            await asyncio.sleep(1)

    save_progress(progress)

    elapsed = (datetime.now() - start_time).total_seconds()

    log(f"\n{'='*60}")
    log("ğŸ‰ ì™„ë£Œ!")
    log(f"  ì²˜ë¦¬: {processed_count}ê°œ | ì„±ê³µ: {success_count}ê°œ ({success_count/max(processed_count,1)*100:.1f}%)")
    log(f"  ì‹œê°„: {elapsed/60:.1f}ë¶„ | ì†ë„: {processed_count/max(elapsed,1)*60:.1f}ê°œ/ë¶„")
    log(f"{'='*60}\n")


if __name__ == "__main__":
    asyncio.run(main())
