"""
ìŠ¤íƒœí”„ ë°ì´í„° í¬ë¡¤ë§
ìƒìœ„ 3,000ê°œ ì• ë‹ˆë©”ì´ì…˜ì˜ ì œì‘ì§„ ì •ë³´ ìˆ˜ì§‘ (ê°ë…, ê°ë³¸ê°€, í”„ë¡œë“€ì„œ ë“±)
"""
import os
import sys

from crawler import AnimeCrawler

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ğŸ‘” ìŠ¤íƒœí”„ ë°ì´í„° í¬ë¡¤ë§                                  â•‘
â•‘   ëŒ€ìƒ: ìƒìœ„ 3,000ê°œ ì• ë‹ˆë©”ì´ì…˜                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    crawler = AnimeCrawler()
    crawler.connect()

    try:
        # ì´ë¯¸ ì• ë‹ˆë©”ì´ì…˜ ë°ì´í„°ê°€ ìˆëŠ” ê²ƒë“¤ì— ëŒ€í•´ ìŠ¤íƒœí”„ ì¶”ê°€
        cursor = crawler.conn.cursor()
        cursor.execute("""
            SELECT id, title_romaji
            FROM anime
            ORDER BY popularity DESC
            LIMIT 3000
        """)

        anime_list = cursor.fetchall()
        total = len(anime_list)

        print(f"ğŸ“Š í¬ë¡¤ë§ ëŒ€ìƒ: {total}ê°œ ì• ë‹ˆë©”ì´ì…˜\n")

        success_count = 0
        error_count = 0

        for i, (anime_id, title) in enumerate(anime_list, 1):
            print(f"\n[{i}/{total}] {title} (ID: {anime_id})")

            try:
                # ìŠ¤íƒœí”„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                response = crawler.client.get_anime_staff(anime_id)

                if response and 'Media' in response:
                    anime_data = response['Media']
                    # ìŠ¤íƒœí”„ ì €ì¥
                    staff_edges = anime_data.get('staff', {}).get('edges', [])
                    staff_count = 0
                    for staff_edge in staff_edges:
                        staff_node = staff_edge.get('node')
                        if staff_node:
                            staff_id = staff_node.get('id')
                            role = staff_edge.get('role')

                            if staff_id and staff_id not in crawler.existing_staff_ids:
                                # ìŠ¤íƒœí”„ í…Œì´ë¸”ì— ì €ì¥
                                occupations = staff_node.get('primaryOccupations', [])
                                occupations_str = ','.join(occupations) if occupations else None

                                crawler.conn.execute("""
                                    INSERT OR IGNORE INTO staff (
                                        id, name_full, name_native,
                                        language, image_url,
                                        description, favourites,
                                        primary_occupations
                                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                                """, (
                                    staff_id,
                                    staff_node.get('name', {}).get('full'),
                                    staff_node.get('name', {}).get('native'),
                                    staff_node.get('languageV2'),
                                    staff_node.get('image', {}).get('large'),
                                    staff_node.get('description'),
                                    staff_node.get('favourites'),
                                    occupations_str
                                ))
                                crawler.existing_staff_ids.add(staff_id)

                            # ì• ë‹ˆë©”ì´ì…˜-ìŠ¤íƒœí”„ ê´€ê³„ ì €ì¥
                            if staff_id and role:
                                crawler.conn.execute("""
                                    INSERT OR IGNORE INTO anime_staff (
                                        anime_id, staff_id, role
                                    ) VALUES (?, ?, ?)
                                """, (anime_id, staff_id, role))

                            staff_count += 1

                    crawler.conn.commit()
                    success_count += 1
                    print(f"  âœ… ìŠ¤íƒœí”„ {staff_count}ëª… ì €ì¥")

                time.sleep(1)  # Rate limiting

            except Exception as e:
                error_count += 1
                print(f"  âŒ ì—ëŸ¬: {e}")
                continue

        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ì„±ê³µ: {success_count}ê°œ")
        print(f"  ì‹¤íŒ¨: {error_count}ê°œ")
        print(f"{'='*60}\n")

    finally:
        crawler.close()

if __name__ == '__main__':
    import time
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
