"""
ì¶”ì²œ ë°ì´í„° í¬ë¡¤ë§
ì• ë‹ˆë©”ì´ì…˜ ê°„ì˜ ì¶”ì²œ ê´€ê³„ ìˆ˜ì§‘ (ë¹„ìŠ·í•œ ì‘í’ˆ)
"""
import os
import sys

from crawler import AnimeCrawler

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ğŸ’¡ ì¶”ì²œ ë°ì´í„° í¬ë¡¤ë§                                    â•‘
â•‘   ëŒ€ìƒ: ìƒìœ„ 3,000ê°œ ì• ë‹ˆë©”ì´ì…˜                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    crawler = AnimeCrawler()
    crawler.connect()

    try:
        # ì´ë¯¸ ì• ë‹ˆë©”ì´ì…˜ ë°ì´í„°ê°€ ìˆëŠ” ê²ƒë“¤ì— ëŒ€í•´ ì¶”ì²œ ì¶”ê°€
        cursor = crawler.conn.cursor()
        cursor.execute("""
            SELECT id, title_romaji
            FROM anime
            ORDER BY popularity DESC
            LIMIT 3000
        """)

        anime_list = cursor.fetchall()
        total = len(anime_list)

        print(f"ğŸ“Š í¬ë¡¤ë§ ëŒ€ìƒ: {total}ê°œ ì• ë‹ˆë©”ì´ì…˜\n")

        success_count = 0
        error_count = 0
        total_recommendations = 0

        for i, (anime_id, title) in enumerate(anime_list, 1):
            print(f"\n[{i}/{total}] {title} (ID: {anime_id})")

            try:
                # ì¶”ì²œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì¼ë°˜ get_popular_anime_page ì¿¼ë¦¬ì— í¬í•¨ë˜ì–´ ìˆìŒ)
                # AniList APIì—ì„œ recommendationsë¥¼ ê°€ì ¸ì˜¤ëŠ” ë³„ë„ ì¿¼ë¦¬ í•„ìš”
                query = """
                query ($id: Int) {
                    Media(id: $id) {
                        recommendations(sort: RATING_DESC) {
                            edges {
                                node {
                                    rating
                                    mediaRecommendation {
                                        id
                                        title {
                                            romaji
                                            english
                                            native
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
                """

                variables = {'id': anime_id}
                response = crawler.client._make_request(query, variables)

                if response and 'Media' in response:
                    recommendations = response['Media'].get('recommendations', {}).get('edges', [])
                    rec_count = 0

                    for rec_edge in recommendations[:10]:  # ìƒìœ„ 10ê°œ ì¶”ì²œë§Œ
                        rec_node = rec_edge.get('node')
                        if rec_node:
                            rating = rec_node.get('rating', 0)
                            recommended_media = rec_node.get('mediaRecommendation')

                            if recommended_media:
                                recommended_id = recommended_media.get('id')

                                if recommended_id:
                                    # ì¶”ì²œ ê´€ê³„ ì €ì¥
                                    crawler.conn.execute("""
                                        INSERT OR IGNORE INTO anime_recommendation (
                                            anime_id, recommended_anime_id, rating
                                        ) VALUES (?, ?, ?)
                                    """, (anime_id, recommended_id, rating))

                                    rec_count += 1

                    crawler.conn.commit()
                    success_count += 1
                    total_recommendations += rec_count
                    print(f"  âœ… ì¶”ì²œ {rec_count}ê°œ ì €ì¥")

                time.sleep(1)  # Rate limiting

            except Exception as e:
                error_count += 1
                print(f"  âŒ ì—ëŸ¬: {e}")
                continue

        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ì„±ê³µ: {success_count}ê°œ")
        print(f"  ì‹¤íŒ¨: {error_count}ê°œ")
        print(f"  ì´ ì¶”ì²œ: {total_recommendations}ê°œ")
        print(f"{'='*60}\n")

    finally:
        crawler.close()

if __name__ == '__main__':
    import time
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
